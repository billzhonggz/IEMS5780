{"cells":[{"cell_type":"markdown","metadata":{},"outputs":[],"source":["# IEMS5780 - Assignment 1"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":"> 1155130306 Junru Zhong 鍾鈞儒\n>\n> Last modified Oct 13, 2019"},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":"# Imports\nimport glob\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":"## 1. Data Preparation\n\nThe following function is used to do the combining and spliting work. It returns a tuple of pandas dataframes, which corresponding to training and test set.\n\nBecause I am working on a Windows machine, the paths were written in black slashes '\\'. If you are using a Unix machine, please modify them to the slashes '/'."},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":"def combine(dataset_path, is_shuffle=False, save_path=None):\n    \"\"\"Combine the train and test dataset.\n    :param: dataset_path: str\n    :param: is_shuffle: boolean\n    :param: save_path: str, None for don't save\n    :return: (training_dataframe, test_dataframe): tuple\n    \"\"\"\n    print('Date pre-processing...')\n    data = []\n    # Open files in positive comments.\n    for filename in glob.glob(dataset_path + 'train\\\\pos\\\\*.txt'):\n        with open(filename, 'r', encoding='utf8') as f:\n            data += [[f.read().strip(), 1]]\n    for filename in glob.glob(dataset_path + 'test\\\\pos\\\\*.txt'):\n        with open(filename, 'r', encoding='utf8') as f:\n            data += [[f.read().strip(), 1]]\n    # Open files in negative comments.\n    for filename in glob.glob(dataset_path + 'train\\\\neg\\\\*.txt'):\n        with open(filename, 'r', encoding='utf8') as f:\n            data += [[f.read().strip(), 0]]\n    for filename in glob.glob(dataset_path + 'test\\\\neg\\\\*.txt'):\n        with open(filename, 'r', encoding='utf8') as f:\n            data += [[f.read().strip(), 0]]\n\n    # Load datalist into DataFrame\n    df = pd.DataFrame(data, columns=['comment', 'attitude'])\n    # Shuffle\n    if is_shuffle:\n        df = df.sample(frac=1)\n    # Split the dataset\n    df_train, df_test = train_test_split(df, test_size=0.3)\n    # Save DataFrame to csv file.\n    if save_path is not None:\n        with open(save_path + 'train.csv', 'w', encoding='utf8') as f:\n            df_train.to_csv(f)\n        with open(save_path + 'test.csv', 'w', encoding='utf8') as f:\n            df_test.to_csv(f)\n    # Return the dataframe.\n    return df_train, df_test"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":"## 2. Using a Naive Bayes Classification\n\nIn this section, a pipeline will be built to read the data, then count it by `CountVectorizer` and `TfidfVectorizer`, then train a Naive Bayes Classifier."},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":"def naive_bayes_count(train, test):\n    \"\"\"Train a Naive Bayes classifier with count vectorizer.\n    :param training set. pandas Dataframe.\n    :param test set. pandas Dataframe.\n    :param model save path. str. None for don't save.\n    :return sklearn model.\n    \"\"\"\n    print('Training Naive Bayes model with unigram CountVectorize...')\n    # Extract documents and labels.\n    docs_train = train['comment']\n    labels_train = train['attitude']\n    docs_test = test['comment']\n    labels_test = test['attitude']\n    # Start up a Pipeline\n    pipe = Pipeline([\n        ('vec', CountVectorizer()),\n        ('nb', MultinomialNB())\n    ])\n    # Train the model.\n    pipe.fit(docs_train, labels_train)\n    # Do prediction.\n    y_pred = pipe.predict(docs_test)\n    # Get report.\n    print(classification_report(labels_test, y_pred))"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"Use `TfidfVectorizer`"},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":"def naive_bayes_tfidf(train, test):\n    \"\"\"Train a Naive Bayes classifier with Tf-Idf vectorizer.\n    :param training set. pandas Dataframe.\n    :param test set. pandas Dataframe.\n    :param model save path. str. None for don't save.\n    :return sklearn model.\n    \"\"\"\n    print('Training Naive Bayes model with unigram TfidfVectorize...')\n    # Extract documents and labels.\n    docs_train = train['comment']\n    labels_train = train['attitude']\n    docs_test = test['comment']\n    labels_test = test['attitude']\n    # Start up a Pipeline\n    pipe = Pipeline([\n        ('vec', TfidfVectorizer()),\n        ('nb', MultinomialNB())\n    ])\n    # Train the model.\n    pipe.fit(docs_train, labels_train)\n    # Do prediction.\n    y_pred = pipe.predict(docs_test)\n    # Get report.\n    print(classification_report(labels_test, y_pred))"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["## 3. Using Logistic Regression\n","\n","In this section, a pipeline will be built to read the data, then count it by CountVectorizer and TfidfVectorizer, then train a logistic regression classifier."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":"def logistic_regression_count(train, test):\n    \"\"\"Train a logistic regression classifier with count vectorizer.\n    :param training set. pandas Dataframe.\n    :param test set. pandas Dataframe.\n    :param model save path. str. None for don't save.\n    :return sklearn model.\n    \"\"\"\n    print('Training Logistic Regression model with unigram CountVectorize...')\n    # Extract documents and labels.\n    docs_train = train['comment']\n    labels_train = train['attitude']\n    docs_test = test['comment']\n    labels_test = test['attitude']\n    # Start up a Pipeline\n    pipe = Pipeline([\n        ('vec', CountVectorizer()),\n        ('log', LogisticRegression())\n    ])\n    # Train the model.\n    pipe.fit(docs_train, labels_train)\n    # Do prediction.\n    y_pred = pipe.predict(docs_test)\n    # Get report.\n    print(classification_report(labels_test, y_pred))"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"Use `TfidfVectorizer`."},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":"def logistic_regression_tfidf(train, test):\n    \"\"\"Train a logistic regression classifier with Tf-idf vectorizer.\n    :param training set. pandas Dataframe.\n    :param test set. pandas Dataframe.\n    :param model save path. str. None for don't save.\n    :return sklearn model.\n    \"\"\"\n    print('Training Logistic Regression model with unigram TfidfVectorize...')\n    # Extract documents and labels.\n    docs_train = train['comment']\n    labels_train = train['attitude']\n    docs_test = test['comment']\n    labels_test = test['attitude']\n    # Start up a Pipeline\n    pipe = Pipeline([\n        ('vec', TfidfVectorizer()),\n        ('log', LogisticRegression())\n    ])\n    # Train the model.\n    pipe.fit(docs_train, labels_train)\n    # Do prediction.\n    y_pred = pipe.predict(docs_test)\n    # Get report.\n    print(classification_report(labels_test, y_pred))"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["## 4. Bi-Gram Models\n","Repeat all experiments with bi-gram."]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["### Naive Bayes Models"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":"def naive_bayes_count_bigram(train, test):\n    \"\"\"Train a Naive Bayes classifier with count vectorizer.\n    :param training set. pandas Dataframe.\n    :param test set. pandas Dataframe.\n    :param model save path. str. None for don't save.\n    :return sklearn model.\n    \"\"\"\n    print('Training Naive Bayes model with bigram CountVectorize...')\n    # Extract documents and labels.\n    docs_train = train['comment']\n    labels_train = train['attitude']\n    docs_test = test['comment']\n    labels_test = test['attitude']\n    # Start up a Pipeline\n    pipe = Pipeline([\n        ('vec', CountVectorizer(ngram_range=(1,2))),\n        ('nb', MultinomialNB())\n    ])\n    # Train the model.\n    pipe.fit(docs_train, labels_train)\n    # Do prediction.\n    y_pred = pipe.predict(docs_test)\n    # Get report.\n    print(classification_report(labels_test, y_pred))"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":"### Logistic Regression Models"},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":"def logistic_regression_count_bigram(train, test):\n    \"\"\"Train a logistic regression classifier with count vectorizer.\n    :param training set. pandas Dataframe.\n    :param test set. pandas Dataframe.\n    :param model save path. str. None for don't save.\n    :return sklearn model.\n    \"\"\"\n    print('Training Logistic Regression model with biigram CountVectorize...')\n    # Extract documents and labels.\n    docs_train = train['comment']\n    labels_train = train['attitude']\n    docs_test = test['comment']\n    labels_test = test['attitude']\n    # Start up a Pipeline\n    pipe = Pipeline([\n        ('vec', CountVectorizer(ngram_range=(1,2))),\n        ('log', LogisticRegression())\n    ])\n    # Train the model.\n    pipe.fit(docs_train, labels_train)\n    # Do prediction.\n    y_pred = pipe.predict(docs_test)\n    # Get report.\n    print(classification_report(labels_test, y_pred))"},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":"# Data preprocessing Please fill your path of dataset and output file.\ntrain, test = combine('D:\\\\Datasets\\\\aclImdb\\\\', True, None)\n# Run all models.\nnaive_bayes_count(train, test)\nnaive_bayes_tfidf(train, test)\nlogistic_regression_count(train, test)\nlogistic_regression_tfidf(train, test)\nnaive_bayes_count_bigram(train, test)\nlogistic_regression_count_bigram(train, test)"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"## Models Compares\n\n| Model | Accuracy | Precision (pos) | Recall (pos) | Percision (neg) | Recall (neg) |\n| :---: | :------: | :-------------: | :----------: | :-------------: | :----------: |\n| NB-Unigram | 0.84 | 0.87 | 0.81 | 0.82 | 0.88 |\n| NB-Tfidf | 0.86 | 0.88 | 0.83 | 0.84 | 0.88 |\n| Logistic-Unigram | 0.88 | 0.88 | 0.89 | 0.89 | 0.88 |\n| Logistic-Tfidf | 0.89 | 0.89 | 0.90 | 0.90 | 0.88 |\n| NB-Bigram | 0.88 | 0.89 | 0.86 | 0.87 | 0.90 |\n| Logistic-Bigram | 0.91 | 0.90 | 0.91 | 0.91 | 0.90 |\n\nFrom the results above, we can see the best-performed model was **Logistics Regression model with bigram CountVectorizer**. So this model will be saved on the later section and used for the Telegram chatbot.\n"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":"## 5. fastText\nNow train a fastText model on the movie comments."},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":"# Preprocess the data by fastText format.\nimport csv\n\ndef pre_process_fasttext(dataset_path, save_path):\n    \"\"\"Dump training set and test set from txt files with labels.\n    :param path to dataset. str\n    :param path to save the processed data. str\n    \"\"\"\n    data = []\n    # Open files in positive comments.\n    for filename in glob.glob(dataset_path + 'train\\\\pos\\\\*.txt'):\n        with open(filename, 'r', encoding='utf8') as f:\n            data += [['__label__positive ' + f.read().strip()]]\n    for filename in glob.glob(dataset_path + 'test\\\\pos\\\\*.txt'):\n        with open(filename, 'r', encoding='utf8') as f:\n            data += [['__label__positive ' + f.read().strip()]]\n    # Open files in negative comments.\n    for filename in glob.glob(dataset_path + 'train\\\\neg\\\\*.txt'):\n        with open(filename, 'r', encoding='utf8') as f:\n            data += [['__label__negaitive ' + f.read().strip()]]\n    for filename in glob.glob(dataset_path + 'test\\\\neg\\\\*.txt'):\n        with open(filename, 'r', encoding='utf8') as f:\n            data += [['__label__negaitive ' + f.read().strip()]]\n    \n    # Load datalist into DataFrame\n    df = pd.DataFrame(data, columns=['comment_label'])\n    df = df.sample(frac=1)\n    # Split the dataset\n    df_train, df_test = train_test_split(df, test_size=0.3)\n    # Save DataFrame to csv file.\n    with open(save_path + 'train.txt', 'w', encoding='utf8') as f:\n        df_train.to_csv(f, header=None, index=None, mode='a', quoting=csv.QUOTE_NONE, escapechar='\\\\')\n    with open(save_path + 'test.txt', 'w', encoding='utf8') as f:\n        df_test.to_csv(f, header=None, index=None, mode='a', quoting=csv.QUOTE_NONE, escapechar='\\\\')\n\n# Change to your dataset path.\npre_process_fasttext('D:\\\\Datasets\\\\aclImdb\\\\', 'D:\\\\Datasets\\\\aclImdb\\\\fastText\\\\')"},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":"# Train a fastText model.\nfrom fasttext import train_supervised\n\ndef train_fasttext(train_path, test_path, epoch, learning_rate, n_gram):\n    model = train_supervised(\n        input=train_path,\n        epoch=epoch,\n        lr=learning_rate,\n        wordNgrams=n_gram,\n        verbose=2,\n        minCount=1\n    )\n    print(model.test(test_path))\n    return model\n\n# Call the train function\nft_model = train_fasttext('D:\\\\Datasets\\\\aclImdb\\\\fastText\\\\train.txt', 'D:\\\\Datasets\\\\aclImdb\\\\fastText\\\\test.txt', 25, 1, 2)\n# Save the model to current directory\nft_model.save_model('imdb_comments_ft.bin')"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"From the test output, we can see the accruacy is around 0.89."},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"## 6. Model Saving\n\nFrom the last section, the bigram logistic regression model has the best performance. So let's add the save statements on the code then save this model."},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":"from joblib import dump\n\ndef logistic_regression_count_bigram_save(train, test, path):\n    \"\"\"Train a logistic regression classifier with count vectorizer.\n    :param training set. pandas Dataframe.\n    :param test set. pandas Dataframe.\n    :param model save path. str. None for don't save.\n    :return sklearn model.\n    \"\"\"\n    print('Training Logistic Regression model with biigram CountVectorize...')\n    # Extract documents and labels.\n    docs_train = train['comment']\n    labels_train = train['attitude']\n    docs_test = test['comment']\n    labels_test = test['attitude']\n    # Start up a Pipeline\n    pipe = Pipeline([\n        ('vec', CountVectorizer(ngram_range=(1,2))),\n        ('log', LogisticRegression())\n    ])\n    # Train the model.\n    pipe.fit(docs_train, labels_train)\n    # Do prediction.\n    y_pred = pipe.predict(docs_test)\n    # Get report.\n    print(classification_report(labels_test, y_pred))\n    dump(pipe, path)\n\nlogistic_regression_count_bigram_save(train, test, 'model.joblib')"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}